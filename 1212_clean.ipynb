{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Study of Implicit Bias in Machine Learning\n",
    "CS 760: Machine Learning final project (Fall 2020)\n",
    "\n",
    "Bella Bai, Liu Yang, Xinyi Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for experiments\n",
    "We will load packages and data in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "# Input data files are available in the \"./input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results  written to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import tree, linear_model, neighbors, naive_bayes, ensemble\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Configure Visualization Defaults\n",
    "# %matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import data from file\n",
    "data_raw = pd.read_csv('./input/propublica-violent-recidivism.csv')\n",
    "\n",
    "#to play with our data we'll create a copy\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "data_cleaner = [data1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "In this section, we preprocess the data, namely:\n",
    "- select the needed features for training and testing\n",
    "- complete the missing or invalid value in the dataset\n",
    "- encode the feature with one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select features needed for training\n",
    "features = [\n",
    "    'sex',\n",
    "    'age_cat',\n",
    "    'race',\n",
    "    'juv_fel_count',\n",
    "    'decile_score',\n",
    "    'juv_misd_count',\n",
    "    'juv_other_count',\n",
    "    'priors_count',\n",
    "    'c_charge_degree',\n",
    "    'is_recid',\n",
    "    'score_text',\n",
    "    'v_score_text',\n",
    "    'two_year_recid'\n",
    "]\n",
    "data1 = data1[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compte or delete missing values in the data\n",
    "data1['score_text'].fillna(data1['score_text'].mode()[0], inplace = True)\n",
    "assert data1.isnull().sum().all() == 0, \"There should be no NAN value in the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the data using one-hot.\n",
    "data_dummy = pd.get_dummies(data1, prefix=['sex', 'age_cat', 'race', 'c_charge_degree', 'score_text', 'v_score_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n",
    "\n",
    "#code categorical data\n",
    "label = LabelEncoder()\n",
    "# label encoder mark the sample according to its order:\n",
    "# for example: LabelEncoder.fit_transform([1, 1, 100, 67, 5]) will get ([0, 0, 3, 2, 1])\n",
    "\n",
    "# Sex: Male, Female => 0, 1\n",
    "data1['sex_code'] = label.fit_transform(data1['sex'])\n",
    "# age_cat (age category): 'Greater than 45', '25 - 45', 'Less than 25' => 1, 0, 2\n",
    "data1['age_cat_code'] = label.fit_transform(data1['age_cat'])\n",
    "# race: 'Other', 'African-American', 'Caucasian', 'Hispanic', 'Asian', 'Native American\n",
    "data1['race_code'] = label.fit_transform(data1['race'])\n",
    "# c_charge_degree: 'F', 'M'\n",
    "data1['c_charge_degree_code'] = label.fit_transform(data1['c_charge_degree'])\n",
    "data1['score_text_code'] = label.fit_transform(data1['score_text'])\n",
    "data1['v_score_text_code'] = label.fit_transform(data1['v_score_text'])\n",
    "\n",
    "#define y variable aka target/outcome\n",
    "Target = ['two_year_recid']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = features[:-1]\n",
    "data1_x_calc = [\n",
    "    'sex_code',\n",
    "    'age_cat_code',\n",
    "    'race_code',\n",
    "    'juv_fel_count',\n",
    "    'decile_score',\n",
    "    'juv_misd_count',\n",
    "    'juv_other_count',\n",
    "    'priors_count',\n",
    "    'c_charge_degree_code',\n",
    "    'is_recid',\n",
    "    'score_text_code', \n",
    "    'v_score_text_code',\n",
    "] \n",
    "data1_xy =  Target + data1_x\n",
    "#define x variables for dummied features aka real feature selection\n",
    "features_dummied = list(data_dummy.columns)\n",
    "data1_x_dummied = list(data_dummy.columns)\n",
    "data1_x_dummied.remove('two_year_recid')\n",
    "data1_x_cal_dummied = data1_x_dummied\n",
    "data1_xy_dummied = Target + data1_x_dummied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the train and test dataset\n",
    "# train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], test_size=0.25, random_state = 0)\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data_dummy[data1_x_cal_dummied], data_dummy[Target], test_size=0.25, random_state = 0)\n",
    "\n",
    "# get the corresponding raw dataset for later use\n",
    "train1_x_raw, test1_x_raw, train1_y_raw, test1_y_raw = model_selection.train_test_split(data1, data1[Target], test_size=0.25, random_state = 0)\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "train1_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation on the data\n",
    "In this section, we observe the data property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete Variable Correlation by Survival using\n",
    "#group by aka pivot table\n",
    "for x in data1_x:\n",
    "    if data1[x].dtype != 'float64' :\n",
    "        print('two_year_recid Correlation by:', x)\n",
    "        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n",
    "        print('-'*10, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the percentage of each group in race feature\n",
    "sensitive = test1_x_raw[\"race\"]  # only look at sample of feature \"race\"\n",
    "sensitive_values = list(set(sensitive))\n",
    "# the list is ['Other', 'Hispanic', 'African-American', 'Asian', 'Native American', 'Caucasian']\n",
    "counting = []\n",
    "name = []\n",
    "for sens in sensitive_values:\n",
    "    idx = sensitive == sens\n",
    "    n_g = idx.sum()\n",
    "    counting.append(n_g)\n",
    "    name.append(sens)\n",
    "\n",
    "plt.figure(figsize=[9,9])\n",
    "plt.pie(counting,labels=name,autopct='%1.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.savefig(\"pie_chart_race.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axis1,axis2,axis3) = plt.subplots(1, 3,figsize=(16,12))\n",
    "\n",
    "sns.barplot(x = 'race', y = 'two_year_recid', data=data1, ax = axis1)\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid',  data=data1, ax = axis2)\n",
    "sns.barplot(x = 'age_cat', y = 'two_year_recid', data=data1, ax = axis3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'race', data=data1, ax = qaxis[0])\n",
    "axis1.set_title('sex vs race two_year_recid Comparison')\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'age_cat', data=data1, ax  = qaxis[1])\n",
    "axis1.set_title('sex vs age_cat two_year_recid Comparison')\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'score_text_code', data=data1, ax  = qaxis[2])\n",
    "axis1.set_title('sex vs score_text_code two_year_recid Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Learning Algorithms\n",
    "In this section, we train and compare the basic machine learning algorithms include:\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- Bernoulli Naive Bayes\n",
    "- Gaussian Naive Bayes\n",
    "- k Nearest Neighborhood\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    # ensembles\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(), # prior is bernoulli\n",
    "    naive_bayes.GaussianNB(),  # prior is gaussian\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier()  \n",
    "    ]\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = test1_y[Target]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    \n",
    "    #score model with cross validation\n",
    "    alg.fit(train1_x, train1_y)\n",
    "    MLA_predict[MLA_name] = alg.predict(test1_x) #alg.predict(data1[data1_x_calc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick candidate model to analysis its confusion matrix under two subgroups:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# caucassian women\n",
    "data2 = test1_x_raw[(test1_x_raw['sex'] == 'Female') & (test1_x_raw['race'] == 'Caucasian')]\n",
    "actual2 = data2[Target]\n",
    "# african_america men\n",
    "data3 = test1_x_raw[(test1_x_raw['sex'] == 'Male') & (test1_x_raw['race'] == 'African-American')]\n",
    "actual3 = data3[Target]\n",
    "\n",
    "for alg in MLA:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2,figsize=(16,12))\n",
    "    pred = MLA_predict[alg.__class__.__name__].copy()\n",
    "    pred = pred.loc[actual2.index.intersection(pred.index)]\n",
    "    conf_mat = confusion_matrix(actual2, pred)\n",
    "    print(conf_mat)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "#     ax = fig.add_subplot(211)\n",
    "    cax = ax1.matshow(conf_mat, interpolation='nearest')\n",
    "#     fig.colorbar(cax)\n",
    "    ax1.set_title(alg.__class__.__name__ + '\\'s For Female Caucasian')\n",
    "\n",
    "    pred = MLA_predict[alg.__class__.__name__].copy()\n",
    "    pred = pred.loc[actual3.index.intersection(pred.index)]\n",
    "#     plt.subplot(1, 10, MLA.index(alg) + 1)\n",
    "    conf_mat = confusion_matrix(actual3, pred)\n",
    "    print(conf_mat)\n",
    "    conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "#     ax = fig.add_subplot(212)\n",
    "    cax = ax2.matshow(conf_mat, interpolation='nearest')\n",
    "#     fig.colorbar(cax)\n",
    "    ax2.set_title(alg.__class__.__name__ + '\\'s For Male African-American')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Metric Measurements\n",
    "We will include measurements of the following:\n",
    "- accuracy\n",
    "- equal opportunity\n",
    "- disparate impact (DI) score\n",
    "- generalized entropy index (alpha = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "obj = metrics(data=test1_x_raw, prediction=MLA_predict)\n",
    "\n",
    "# plot figure\n",
    "def plot_figure(score_dict, score_name, ylim=[0, 1]):\n",
    "    name_dict = {'RandomForestClassifier': 'RandomForest',\n",
    "     'LogisticRegressionCV': 'Logistic',\n",
    "     'BernoulliNB': \"BernoulliNB\",\n",
    "     'GaussianNB': \"GaussianNB\",\n",
    "     'KNeighborsClassifier': \"KNN\",\n",
    "     'DecisionTreeClassifier': \"DT\"\n",
    "    }\n",
    "    score_list = []\n",
    "    name_list = []\n",
    "    # obj.algs is the list of algorithm we measure\n",
    "    for alg in obj.algs:\n",
    "        score_list.append(score_dict[alg])\n",
    "        name_list.append(name_dict[alg])\n",
    "\n",
    "    pylab.rcParams['figure.figsize'] = 12,6\n",
    "    sns.barplot(x=name_list, y=score_list, palette=\"Blues_d\")\n",
    "    sns.set(font_scale = 3)\n",
    "    plt.title(score_name)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(\"baseline_{}.pdf\".format(score_name), bbox_inches='tight')\n",
    "    \n",
    "acc_score = obj.get_accuracy()\n",
    "equal_score = obj.get_equality_opportunity()\n",
    "DI_score = obj.get_DI_score()\n",
    "GEI_score = obj.get_generalized_entropy_index()\n",
    "GEI_within_score = obj.get_generalized_entropy_index_within_group()\n",
    "GEI_between_score = obj.get_generalized_entropy_index_between_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(equal_score, \"Equal Opportunity\", [0.9, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_di_figure(acc_dict, di_dict, threshold, name):\n",
    "    name_dict = {'RandomForestClassifier': 'RandomForest',\n",
    "     'LogisticRegressionCV': 'Logistic',\n",
    "     'BernoulliNB': \"BernoulliNB\",\n",
    "     'GaussianNB': \"GaussianNB\",\n",
    "     'KNeighborsClassifier': \"KNN\",\n",
    "     'DecisionTreeClassifier': \"DT\"\n",
    "    }\n",
    "    acc_list = []\n",
    "    di_list = []\n",
    "    name_list = []\n",
    "    # obj.algs is the list of algorithm we measure\n",
    "    for alg in obj.algs:\n",
    "        acc_list.append(acc_dict[alg])\n",
    "        di_list.append(di_dict[alg])\n",
    "        name_list.append(name_dict[alg])\n",
    "\n",
    "    pylab.rcParams['figure.figsize'] = 9,9\n",
    "    sns.scatterplot(x=di_list, y=acc_list, hue=name_list, style=name_list, s=300)\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.xlabel(\"DI score\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim([0.85, 0.96])\n",
    "    plt.xlim([0.85, 0.96])\n",
    "    plt.axvline(x=threshold, ls='--', c='b')\n",
    "    plt.title(name)\n",
    "    plt.savefig(\"baseline_{}.pdf\".format(name), bbox_inches='tight')\n",
    "\n",
    "# calculate the data property\n",
    "S_1 = test1_x_raw[test1_x_raw['race'] == 'Caucasian']\n",
    "S_1_Y_1 = S_1[S_1['two_year_recid'] == 0]\n",
    "S_not_1 = test1_x_raw[test1_x_raw['race'] != 'Caucasian']\n",
    "S_not_1_Y_1 = S_not_1[S_not_1['two_year_recid'] == 0]\n",
    "threshold = (S_1.shape[0] * S_not_1_Y_1.shape[0]) / (S_1_Y_1.shape[0] * S_not_1.shape[0])\n",
    "    \n",
    "plot_acc_di_figure(acc_score, DI_score, threshold, \"Comparison between accuracy and DI score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(GEI_within_score, \"GEI within-group unfairness\", [0.0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure(GEI_between_score, \"GEI between-group unfairness\", [0.0, 0.005])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Enhancing Methods\n",
    "In the following, we will apply the fairness enhancing methods:\n",
    "- preprocessing: Feldman et al.\n",
    "- inprocessing: meta classifier\n",
    "- postprocessing: calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "    .. [1] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and\n",
    "       S. Venkatasubramanian, \"Certifying and removing disparate impact.\"\n",
    "       ACM SIGKDD International Conference on Knowledge Discovery and Data\n",
    "       Mining, 2015.\n",
    "\"\"\"\n",
    "# Run Feldman to repair our data set: repaired feature = sex, race\n",
    "from BlackBoxAuditing.repairers.GeneralRepairer import Repairer\n",
    "all_data = data1.values.tolist()\n",
    "feature_list = data1.keys().to_list()\n",
    "feature_to_repair = feature_list.index('sex')\n",
    "repairer = Repairer(all_data, feature_to_repair, 0.8, False)\n",
    "repaired_sex_data = repairer.repair(all_data)\n",
    "feature_to_repair = feature_list.index('race')\n",
    "repairer = Repairer(repaired_sex_data, feature_to_repair, 1.0, False)\n",
    "repaired_data = repairer.repair(repaired_sex_data)\n",
    "print(repaired_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert repaired_data (list) ro df\n",
    "repaired_df = pd.DataFrame(repaired_data, columns=feature_list)\n",
    "print(repaired_df.info())\n",
    "repaired_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repaired_df['two_year_recid'] = data1['two_year_recid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, qaxis = plt.subplots(1,3,figsize=(14,12))\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'race', data=repaired_df, ax = qaxis[0])\n",
    "axis1.set_title('sex vs race two_year_recid Comparison')\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'age_cat', data=repaired_df, ax  = qaxis[1])\n",
    "axis1.set_title('sex vs age_cat two_year_recid Comparison')\n",
    "\n",
    "sns.barplot(x = 'sex', y = 'two_year_recid', hue = 'score_text_code', data=repaired_df, ax  = qaxis[2])\n",
    "axis1.set_title('sex vs score_text_code two_year_recid Comparison')\n",
    "\n",
    "# repair makes all male, and all african-american"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot the reparied dataset and drop the generated labelEncoder one\n",
    "repaired_df_dummy = pd.get_dummies(repaired_df, prefix=['sex', 'age_cat', 'race', 'c_charge_degree', 'score_text', 'v_score_text'])\n",
    "repaired_x_cal_dummied  = repaired_df_dummy.columns.to_list()\n",
    "removed_items = ['sex_code', 'age_cat_code', 'race_code', 'c_charge_degree_code', 'score_text_code', 'v_score_text_code' ,'two_year_recid']\n",
    "for item in removed_items:\n",
    "    print(item)\n",
    "    repaired_x_cal_dummied.remove(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_x_cal_dummied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repaired_train1_x, repaired_test1_x, repaired_train1_y, repaired_test1_y = model_selection.train_test_split(repaired_df_dummy[repaired_x_cal_dummied], repaired_df_dummy[Target], test_size=0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repaired_train1_x_raw, repaired_test1_x_raw, repaired_train1_y_raw, repaired_test1_y_raw = model_selection.train_test_split(repaired_df, repaired_df[Target], test_size=0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_train1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table to compare metrics after repair\n",
    "# MLA_compare_after = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict_after = repaired_test1_y[Target]\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(repaired_train1_x, train1_y)\n",
    "    MLA_predict_after[MLA_name] = alg.predict(repaired_test1_x)#alg.predict(repaired_df[data1_x_calc])\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "\n",
    "    \n",
    "# #print and sort table:\n",
    "# MLA_compare_after.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "# MLA_compare_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the DIavgall again\n",
    "\n",
    "# commented out bc they were initialized above\n",
    "# actual = repaired_test1_x_raw[Target]\n",
    "# dict_of_sensitive_lists = repaired_test1_x_raw\n",
    "# dict_of_sensitive_lists = test1_x_raw\n",
    "# single_sensitive_name = 'race'\n",
    "# unprotected_vals = ['African-American', 'Caucasian']\n",
    "# unprotected_vals = ['Caucasian']\n",
    "# positive_pred = 1\n",
    "def calc(actual, predicted, dict_of_sensitive_lists, single_sensitive_name,\n",
    "             unprotected_vals, positive_pred):\n",
    "    \"\"\"\n",
    "    :param actual: label\n",
    "    :param predicted: prediction\n",
    "    :param dict_of_sensitive_lists: samples\n",
    "    :param single_sensitive_name: focus on which feature, here is race\n",
    "    :param unprotected_vals: ['African-American', 'Caucasian']\n",
    "    :param positive_pred: 1, means that person has recid in two years\n",
    "    \"\"\"\n",
    "    sensitive = dict_of_sensitive_lists[single_sensitive_name]  # only look at sample of feature \"race\"\n",
    "    sensitive_values = list(set(sensitive))\n",
    "    # the list is ['Other', 'Hispanic', 'African-American', 'Asian', 'Native American', 'Caucasian']\n",
    "    \n",
    "    if len(sensitive_values) <= 1:\n",
    "        print(\"ERROR: Attempted to calculate DI without enough sensitive values:\" + \\\n",
    "               str(sensitive_values))\n",
    "        return 1.0\n",
    "\n",
    "    # this list should only have one item in it\n",
    "    what_is_this = [val for val in sensitive_values if val in unprotected_vals]\n",
    "    # ['African-American', 'Caucasian']\n",
    "    single_unprotected = [val for val in sensitive_values if val in unprotected_vals][0]\n",
    "#     print(single_unprotected)\n",
    "    # single_unprotected becomes only 'African-American'. Something wrong?\n",
    "    unprotected_prob = calc_prob_class_given_sensitive(predicted, sensitive, positive_pred,\n",
    "                                                       single_unprotected)\n",
    "\n",
    "    sensitive_values.remove(single_unprotected)\n",
    "    total = 0.0\n",
    "    for sens in sensitive_values:\n",
    "        pos_prob = calc_prob_class_given_sensitive(predicted, sensitive, positive_pred, sens)\n",
    "        DI = 0.0\n",
    "        if unprotected_prob > 0:\n",
    "            DI = pos_prob / unprotected_prob\n",
    "        if unprotected_prob == 0.0 and pos_prob == 0.0:\n",
    "            DI = 1.0\n",
    "        total += DI\n",
    "\n",
    "    if total == 0.0:\n",
    "         return 1.0\n",
    "\n",
    "    return total / len(sensitive_values)\n",
    "\n",
    "\n",
    "x_axis = ['RandomForest', 'Logistic', 'BernoulliNB', 'GaussianNB', 'KNN', 'DT']\n",
    "\n",
    "DI_res_repaired = []\n",
    "actual = repaired_test1_x_raw[Target]\n",
    "dict_of_sensitive_lists = test1_x_raw\n",
    "single_sensitive_name = 'sex'\n",
    "unprotected_vals = ['Male']\n",
    "positive_pred = 1\n",
    "for alg in MLA:\n",
    "    predicted = MLA_predict_after[alg.__class__.__name__]\n",
    "    score = calc(actual, predicted, dict_of_sensitive_lists, single_sensitive_name, unprotected_vals, positive_pred)\n",
    "    DI_res_repaired.append(score)\n",
    "    print(alg.__class__.__name__ + \"'s DI score': \" + str(score))\n",
    "\n",
    "    \n",
    "DI_res = []\n",
    "for alg in MLA:\n",
    "    predicted = MLA_predict[alg.__class__.__name__]\n",
    "    score = calc(actual, predicted, dict_of_sensitive_lists, single_sensitive_name, unprotected_vals, positive_pred)\n",
    "    DI_res.append(score)\n",
    "    print(alg.__class__.__name__ + \"'s DI score': \" + str(score))\n",
    "\n",
    "    \n",
    "# plot\n",
    "plt.plot(x_axis, DI_res, label=\"baseline models\")\n",
    "plt.plot(x_axis, DI_res_repaired, label=\"baseline+Feldman\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# the current DI calculation looks skecthy, looking for calibration (group condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all race features after one hot\n",
    "data_race = data1.copy(deep=True).race\n",
    "data_race_cols =   pd.get_dummies(data_race).columns.to_list()\n",
    "for i in range(len(data_race_cols)):\n",
    "    data_race_cols[i] = 'race' + \"_\" + data_race_cols[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     \"\"\"The meta algorithm here takes the fairness metric as part of the input\n",
    "#     and returns a classifier optimized w.r.t. that fairness metric [11]_.\n",
    "#     References:\n",
    "#         .. [11] L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi.\n",
    "#            \"Classification with Fairness Constraints: A Meta-Algorithm with\n",
    "#            Provable Guarantees,\" 2018.\n",
    "#     \"\"\"\n",
    "# run Meta Algorithm\n",
    "# import the fairness metric to be used, here we use disparate impact, consistent with feldman above\n",
    "\n",
    "from aif360.algorithms.inprocessing.celisMeta.StatisticalRate import StatisticalRate\n",
    "tau = 0 # fairness penalty parameter\n",
    "# sens_index = data1_x_calc.index('race_code')\n",
    "sens_indexs = []\n",
    "for race in data_race_cols:\n",
    "    sens_indexs.append(data1_x_cal_dummied.index(race))\n",
    "\n",
    "x_train = train1_x.values\n",
    "y_train = np.array([1 if y == 1 else\n",
    "                           -1 for y in train1_y.values])\n",
    "x_control_train = []\n",
    "for i in range(len(x_train)):\n",
    "    for sens_index in sens_indexs:\n",
    "        x_control_train.append(x_train[i][sens_index])\n",
    "# train the meta classifier\n",
    "meta = StatisticalRate().getModel(tau, x_train, y_train, x_control_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_predictions = []\n",
    "for entry in test1_x.values:\n",
    "    t = meta(entry)\n",
    "    meta_predictions.append(int(t > 0))\n",
    "DI_res_meta = calc(actual, meta_predictions, dict_of_sensitive_lists, single_sensitive_name, unprotected_vals, positive_pred)\n",
    "print(\"meta-classifier's DI score: \" + str(DI_res_meta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results from meta algorithm (sens_index = sex_code, tau = 0.8)\n",
    "# Training Accuracy:  0.9443351138599944 , Training gamma:  0.7533624796956752\n",
    "# meta-classifier's DI score: 1.6959727311396713\n",
    "cnt = 0\n",
    "for i in range(len(test1_y.values)):\n",
    "    if meta_predictions[i] != test1_y.values[i]:\n",
    "        cnt += 1\n",
    "meta_acc = 1 - cnt / len(meta_predictions)\n",
    "print(\"meta-classifier's accuracy: \" + str(meta_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing: calibration\n",
    "\"\"\"Calibrated equalized odds postprocessing is a post-processing technique\n",
    "that optimizes over calibrated classifier score outputs to find\n",
    "probabilities with which to change output labels with an equalized odds\n",
    "objective [7]_.\n",
    "References:\n",
    "    .. [7] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and\n",
    "       K. Q. Weinberger, \"On Fairness and Calibration,\" Conference on Neural\n",
    "       Information Processing Systems, 2017\n",
    "Adapted from:\n",
    "https://github.com/gpleiss/equalized_odds_and_calibration/blob/master/calib_eq_odds.py\n",
    "\"\"\"\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "\n",
    "\n",
    "priviledged = [{'race_code': 0}, {'race_code': 1}, {'race_code': 3}, {'race_code': 4}]\n",
    "unpriviledged = [{'race_code': 2}, {'race_code': 5}]\n",
    "calibration = CalibratedEqOddsPostprocessing(unpriviledged, priviledged)\n",
    "\n",
    "from aif360.datasets.binary_label_dataset import BinaryLabelDataset\n",
    "\n",
    "protected_attr_names = ['race_code']\n",
    "unprivileged_protected_attributes = [[2, 5]]\n",
    "privileged_protected_attributes = [[0, 1, 3, 4]]\n",
    "data_true = pd.concat([test1_x, test1_y], axis = 1)\n",
    "dataset_true = BinaryLabelDataset(df=data_true, label_names=Target,\n",
    "        protected_attribute_names=protected_attr_names)\n",
    "cali_res = []\n",
    "for alg in MLA:\n",
    "    data_pred = pd.concat([test1_x, MLA_predict[alg.__class__.__name__]], axis = 1)\n",
    "    data_pred.columns = data1_x_calc+Target\n",
    "    dataset_pred = BinaryLabelDataset(df=data_pred, label_names=Target, protected_attribute_names=protected_attr_names)\n",
    "    cali_res.append(calibration.fit_predict(dataset_true, dataset_pred))\n",
    "# print(cali_res[0].labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate DI score for calibration\n",
    "DI_res_cali = []\n",
    "for i in range(len(MLA)):\n",
    "    predicted = cali_res[i].labels\n",
    "    score = calc(actual, predicted, dict_of_sensitive_lists, single_sensitive_name, unprotected_vals, positive_pred)\n",
    "    DI_res_cali.append(score)\n",
    "    print(MLA[i].__class__.__name__ + \"'s DI score': \" + str(score))\n",
    "\n",
    "# plot\n",
    "plt.plot(x_axis, DI_res, label=\"baseline models\")\n",
    "plt.plot(x_axis, DI_res_cali, label=\"baseline+calibration\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calibration: \")\n",
    "cali_acc = []\n",
    "for j in range(len(cali_res)):\n",
    "    cnt = 0\n",
    "    for i in range(len(test1_y.values)):\n",
    "        if test1_y.values[i] != cali_res[j].labels.tolist()[i]:\n",
    "            cnt += 1\n",
    "    acc = 1 - cnt / len(test1_y.values)\n",
    "    cali_acc.append(acc)\n",
    "    print(MLA[j].__class__.__name__ + \"'s accuracy'\" + str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline:\")\n",
    "base_acc = [0, 0, 0, 0, 0, 0]\n",
    "for j in range(len(MLA)):\n",
    "    for i in range(len(MLA_predict)):\n",
    "        if test1_y.values[i] == MLA_predict[MLA[j].__class__.__name__].values[i]:\n",
    "            base_acc[j] += 1\n",
    "    base_acc[j] /= len(test1_y)\n",
    "    print(MLA[j].__class__.__name__ + \"'s accuracy'\" + str(base_acc[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feldman: \")\n",
    "feldman_acc = [0, 0, 0, 0, 0, 0]\n",
    "for j in range(len(MLA)):\n",
    "    for i in range(len(MLA_predict_after)):\n",
    "        if test1_y.values[i] == MLA_predict_after[MLA[j].__class__.__name__].values[i]:\n",
    "            feldman_acc[j] += 1\n",
    "    feldman_acc[j] /= len(test1_y)\n",
    "    print(MLA[j].__class__.__name__ + \"'s accuracy'\" + str(base_acc[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, base_acc, label=\"baseline models\")\n",
    "plt.plot(x_axis, feldman_acc, label=\"baseline+feldman\")\n",
    "plt.plot(x_axis, cali_acc, label=\"baseline+calibration\")\n",
    "plt.plot(x_axis, [meta_acc] * 6, linestyle=\"--\", label=\"meta classifier\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Accracy Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, DI_res, label=\"baseline models\")\n",
    "plt.plot(x_axis, DI_res_repaired, label=\"baseline+feldman\")\n",
    "plt.plot(x_axis, DI_res_cali, label=\"baseline+calibration\")\n",
    "plt.plot(x_axis, [DI_res_meta] * 6, linestyle=\"--\", label=\"meta classifier\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Fairness Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
